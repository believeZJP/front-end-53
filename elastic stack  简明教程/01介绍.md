## 本身介绍

elastic stack分为4部分

- Kibana：用于数据探索和可视化分析
- Elasticsearch：数据存储、查询与分析
- Beats：数据收集与处理
- Logstash：日志分析

这套工具就是为数据分析而生的，比如搜索引擎，日志分析，指标分析等。



## 常用的名字介绍

es有非常丰富的数据类型：字符串，数值型，布尔，日期，二进制，加上自己独创的范围类型



index相当于数据表的概念（不要再当成数据库了，6.x之后type慢慢被取消了）



Document MetaData

- _index:   文档所在的索引名
- _type: 文档所在的类型名
- _id: 文档唯一的id
- _uid: 组合id，由 _type 和 _id 组成（6.x之后 _type 不起作用，和 _id 一样）
- _source: 文档的原始json数据，可以从这里获取每个字段的内容
- _all: 整合所有字段内容到该字段，默认禁用



es提供restful api做操作

此处加上一些restapi的列子 _TODO_



## ES的倒排索引（Posting list）

es中存储的是一个一个文档内容，当文档存入es中，es会对文档进行拆词然后建立索引，这个索引同时指向原来的文档，这就建立了文档和索引的关系。es的高效搜索也是基于倒排索引实现的。

因此分词是es很重要的组成部分，es中的分词器一共有三部分组成：

- Character Filter：针对原始文本进行处理
- Tokenizer：将原始的文本按照一定规则切分单词
- Token Filters：针对Tokenizer的结果进行再加工

analyzer API可以进行测试某个分词器是否满足自己的需要

```js
POST _analyze
{
  "analyzer": "standard",
  "text": "hello world"
}
```

es自带一些分词器：例如Standard，Simple，Whitespace等，可以根据需求进行分词，并且es还提供30多种多语言分词支持。

如果es的分词器不能满足需求的话，也可以自定义分词器，其实就是代替上面分词器的三个组成部分。同样，es也自带了很多 Tokenizer 和 Token Filters

自定义分词的API

```js
POST text_index
{
  "settings": {
    "analysis": {
      "char_filter": {}, # 定义自己的 char_filter
      "tikenizer": {}, # 定义自己的 tikenizer
      "filter": {}, # 定义自己的 filter
      "analyzer": {} # 在这里引用自己定义的上面三个部分
    }
  }
}
```













